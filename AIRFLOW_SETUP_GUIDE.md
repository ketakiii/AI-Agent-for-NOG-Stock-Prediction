# Airflow Setup and Usage Guide

## üöÄ Quick Start

### 1. Start Docker Desktop
- Open Docker Desktop on your Mac
- Wait for it to fully start (green icon in menu bar)

### 2. Start Airflow
```bash
cd airflow
docker-compose up -d
```

### 3. Access Airflow UI
- **URL**: http://localhost:8080
- **Username**: `admin`
- **Password**: `admin`

## üìã Airflow Commands

### Start/Stop Airflow
```bash
# Start Airflow
docker-compose up -d

# Stop Airflow
docker-compose down

# Restart Airflow
docker-compose down && docker-compose up -d

# View logs
docker-compose logs scheduler
docker-compose logs webserver
```

### Check Status
```bash
# Check if containers are running
docker-compose ps

# Check DAG loading
docker-compose logs scheduler | grep "nog_weekly_prediction"
```

## üéØ Running Your DAGs

### **Production Pipeline** (`nog_production_pipeline`)
**Purpose**: Weekly production runs with smart fallback
**Schedule**: Every Monday at 9 AM (automatic)
**Logic**: 
1. Force data update (try fresh data from Yahoo Finance)
2. If force update fails ‚Üí Run weekly predictions with existing data
3. Send email with results from either path

### **Testing Pipeline** (`nog_testing_pipeline`)
**Purpose**: Manual testing with existing data
**Schedule**: Manual trigger only
**Logic**: Run predictions with existing data (fast, no API calls)

### Method 1: Airflow UI (Recommended)
1. Go to http://localhost:8080
2. Choose your DAG:
   - **Production**: `nog_production_pipeline` (automatic weekly)
   - **Testing**: `nog_testing_pipeline` (manual trigger)
3. Click **"Trigger DAG"** button

### Method 2: Command Line
```bash
# Test pipeline (existing data, fast)
docker-compose exec webserver airflow dags trigger nog_testing_pipeline

# Production pipeline (try fresh, fallback to existing)
docker-compose exec webserver airflow dags trigger nog_production_pipeline
```

## üîß Troubleshooting

### DAG Not Visible
```bash
# Check if DAG loaded successfully
docker-compose logs scheduler | grep "nog_weekly_prediction"

# Check for import errors
docker-compose exec webserver python -c "import sys; sys.path.append('/opt/airflow/project'); from src.models.weekly_predict import WeeklyPredictionPipeline; print('‚úÖ Import successful')"
```

### Tasks Failing
```bash
# Check task logs
docker-compose logs scheduler | tail -50

# Check webserver logs
docker-compose logs webserver | tail -50
```

### Rebuild with Code Changes
```bash
# If you modify the DAG or dependencies
docker-compose down
docker-compose up -d --build
```

## üìÅ File Structure
```
airflow/
‚îú‚îÄ‚îÄ docker-compose.yaml    # Airflow configuration
‚îú‚îÄ‚îÄ Dockerfile            # Custom image with dependencies
‚îú‚îÄ‚îÄ requirements.txt      # Python packages
‚îî‚îÄ‚îÄ dags/
    ‚îî‚îÄ‚îÄ nog_weekly_prediction_dag.py  # Your DAG
```

## üéØ Common Use Cases

### Daily Testing (Use existing data)
1. Trigger DAG with: `{"update_data": false}`
2. Fast execution, no API rate limits

### Weekly Production Run (Fresh data)
1. Trigger DAG with: `{"update_data": true}`
2. Gets latest data from Yahoo Finance
3. May take longer due to API calls

### Force Data Update Only
1. Go to DAG graph view
2. Click on `force_data_update` task
3. Click "Trigger Task"
4. Only updates data, doesn't run predictions

## üîÑ DAG Schedule
- **Schedule**: Every Monday at 9 AM (`0 9 * * 1`)
- **Manual trigger**: Available anytime
- **Configuration**: JSON parameters for data update control

## üìä Monitoring
- **DAG Status**: Green = success, Red = failed
- **Task Logs**: Click on individual tasks to see detailed logs
- **XCom**: Stores results between tasks
- **Email Notifications**: Sent on completion (configure email in DAG)

## üõ†Ô∏è Development Workflow

### 1. Make Code Changes
- Edit your Python files in the main project
- Changes are automatically available in Airflow (volume mount)

### 2. Test Changes
```bash
# Test locally first
python test_simple.py

# Then test in Airflow
# Trigger DAG with {"update_data": false}
```

### 3. Deploy
- No deployment needed - changes are live immediately
- Just restart Airflow if you modify DAG structure

## üö® Important Notes

### üîí **CRITICAL SECURITY WARNING - .env Files**
**NEVER commit .env files to git!** They contain sensitive API keys and passwords.

**What to do:**
- ‚úÖ **ALWAYS** add `.env` files to `.gitignore`
- ‚úÖ **NEVER** commit files containing API keys, passwords, or secrets
- ‚úÖ **ROTATE** API keys immediately if accidentally exposed
- ‚ùå **NEVER** share .env files in code repositories
- ‚ùå **NEVER** include real credentials in documentation

**If you accidentally commit sensitive data:**
1. **IMMEDIATELY** rotate all API keys and passwords
2. **REMOVE** the file from git history using `git filter-branch`
3. **FORCE PUSH** to update remote repository
4. **CONTACT** service providers if keys were exposed

**Example .env file structure (DO NOT commit real values):**
```bash
# airflow/.env - EXAMPLE ONLY (use real values locally)
GMAIL_APP_PASSWORD=your_actual_16_char_password_here
OPENAI_API_KEY=your_actual_openai_key_here
```

### Yahoo Finance Rate Limits
- Use `update_data: false` for frequent testing
- Use `update_data: true` only for weekly production runs
- Yahoo may block requests if called too frequently

### Data Persistence
- Predictions saved to: `data/weekly_predictions.json`
- Performance metrics: `data/model_performance.json`
- Model file: `saved_models/xgb_model.pkl`

### Email Configuration
- Update email address in DAG file
- Currently set to: `your-email@example.com`

## üéâ Success Indicators
- ‚úÖ DAG visible in Airflow UI
- ‚úÖ Tasks complete with green status
- ‚úÖ Predictions generated and saved
- ‚úÖ Email notification received
- ‚úÖ Logs show successful execution

## üìû Quick Reference

| Action | Command |
|--------|---------|
| Start Airflow | `docker-compose up -d` |
| Stop Airflow | `docker-compose down` |
| Check status | `docker-compose ps` |
| View logs | `docker-compose logs scheduler` |
| Test import | `docker-compose exec webserver python -c "import sys; sys.path.append('/opt/airflow/project'); from src.models.weekly_predict import WeeklyPredictionPipeline; print('‚úÖ Import successful')"` |
| **Test Pipeline** (existing data) | `docker-compose exec webserver airflow dags trigger nog_testing_pipeline` |
| **Production Pipeline** (smart fallback) | `docker-compose exec webserver airflow dags trigger nog_production_pipeline` |

---
**Last Updated**: July 19, 2025
**Version**: 1.0 